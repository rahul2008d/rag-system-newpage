# Chat With Your Docs (RAG System)

ğŸš€ **[Live Demo](http://34.245.107.59:8501)**  

---
A containerised **Retrieval-Augmented Generation (RAG)** application that allows users to upload documents (PDF/TXT), index them into a vector store, and ask questions grounded strictly in the uploaded content.

The system is intentionally designed to be **simple, explainable, and production-ready**, prioritising engineering clarity and realistic trade-offs over over-engineering.

---

## ğŸš€ Features

- Upload **PDF / TXT** documents
- Automatic **chunking, embedding, and vector indexing**
- Context-aware Q&A using **OpenAI + FAISS**
- Guardrails to reduce hallucinations (answers only from retrieved context)
- Simple **Streamlit UI**
- Fully **containerised** and deployed on **AWS ECS Fargate**
- Clean engineering standards: linting, formatting, Docker, pre-commit hooks

---

## ğŸ§  Architecture Overview

```
User
 â””â”€â”€ Streamlit UI
      â”œâ”€â”€ Upload documents
      â”œâ”€â”€ Ask questions
      â”‚
      â–¼
RAG System (LangChain-based)
 â”œâ”€â”€ Document Loaders (PDF / TXT)
 â”œâ”€â”€ Text Chunking (overlapping chunks)
 â”œâ”€â”€ OpenAI Embeddings
 â”œâ”€â”€ FAISS Vector Store (local)
 â”œâ”€â”€ Retriever (Top-K similarity)
 â””â”€â”€ OpenAI Chat Completion
```


### Design Principles
- **Retrieval before generation**
- **No answer without relevant context**
- **Stateless UI, stateful vector store**
- **Simple first, extensible later**

---

## ğŸ—ï¸ Tech Stack

| Area | Technology | Reason |
|---|---|---|
| UI | Streamlit | Rapid development, minimal boilerplate |
| RAG Framework | LangChain | Reduces glue code, clean abstractions |
| Embeddings | OpenAI | High-quality semantic representations |
| Vector Store | FAISS | Lightweight, fast, local |
| LLM | OpenAI Chat Models | Reliable, controllable |
| Containerisation | Docker | Reproducible builds |
| Orchestration | AWS ECS Fargate | Serverless containers, low ops |
| Linting | Ruff, Pylint | Consistent code quality |
| CI Hygiene | pre-commit | Catch issues early |

---

## Project Structure

```
Directory structure:
â””â”€â”€ rahul2008d-rag-system-newpage/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ main.py
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ pytest.ini
    â”œâ”€â”€ ruff.toml
    â”œâ”€â”€ .dockerignore
    â”œâ”€â”€ .pre-commit-config.yaml
    â”œâ”€â”€ .pylintrc
    â”œâ”€â”€ .python-version
    â””â”€â”€ src/
        â”œâ”€â”€ app.py
        â””â”€â”€ rag/
            â”œâ”€â”€ __init__.py
            â”œâ”€â”€ chunking.py
            â”œâ”€â”€ config.py
            â”œâ”€â”€ loaders.py
            â”œâ”€â”€ store.py
            â””â”€â”€ system.py

```


---

## âš™ï¸ How It Works (RAG Flow)

### 1. Ingestion
- Load documents using LangChain loaders
- Split text into overlapping chunks
- Generate embeddings using OpenAI
- Store vectors locally in FAISS

### 2. Retrieval
- Embed user query
- Perform Top-K similarity search
- Apply confidence threshold

### 3. Generation
- Build prompt using retrieved context
- Instruct model to answer **only from context**
- Cite chunk IDs inline

If no chunk meets the confidence threshold, the system responds with:

> *"I don't know based on the provided documents."*

---

## ğŸ” Configuration

All configuration is environment-driven.

```
OPENAI_API_KEY=sk-...
EMBEDDING_MODEL=text-embedding-3-small
CHAT_MODEL=gpt-4o-mini
TOP_K=5
MIN_SCORE=0.25
STORE_DIR=vector_store
```

## ğŸ³ Running Locally (Docker)

```
docker build -t chat-with-docs .
docker run -p 8501:8501 --env-file .env chat-with-docs
```

## Open in browser:

```
http://localhost:8501
```

---

## â˜ï¸ Deployment (AWS ECS Fargate)

ğŸš€ **[Open Live Application](https://YOUR_PUBLIC_URL)**

The application is deployed as a **serverless container** using **AWS ECS with the Fargate launch type**, requiring no EC2 or infrastructure management.

### Deployment Setup
- **Container Image**
  - Built using a multi-stage Dockerfile
  - Stored in **Amazon ECR**
  - Versioned image pushed and referenced by ECS task definition

- **ECS Configuration**
  - ECS Cluster with **Fargate** capacity provider
  - ECS Service running a single task (demo-friendly)
  - **0.5 vCPU / 1 GB RAM** per task
  - Public access enabled via **Application Load Balancer**
  - Health checks managed by ECS

- **Runtime Configuration**
  - Environment variables injected at runtime (e.g. `OPENAI_API_KEY`)
  - No secrets baked into the image
  - Stateless container design

- **Persistence & State**
  - Vector store backed by **FAISS** on the container filesystem
  - Re-ingestion required on task restart (intentional trade-off for simplicity)
  - Designed for easy upgrade to S3 / EFS if persistence is required

### Cost & Scaling
- On-demand billing (pay only while the task is running)
- Service can be **paused instantly** by setting:


## Cost Control

- Service can be paused by setting Desired tasks = 0
- No compute cost when stopped
- OpenAI billed strictly per token usage
---

## ğŸ§ª Engineering Standards

- Ruff â€“ linting and formatting
- Pylint â€“ design-focused checks
- pre-commit hooks â€“ enforced locally
- Clear module boundaries
- Minimal hidden magic

## ğŸ”® What Iâ€™d Add With More Time

- Persistent storage (S3 or EFS)
- Authentication and multi-tenant isolation
- Background ingestion jobs

## ğŸ“Œ Summary

This project demonstrates how to build a clean, testable RAG system that can evolve into a larger platform without rewriting core components.